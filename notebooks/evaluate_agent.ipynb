{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Sailing Agents\n",
    "\n",
    "This notebook provides a simple interface for evaluating sailing agents on different scenarios. You can:\n",
    "\n",
    "1. Test your agent on any predefined scenario\n",
    "2. Get quantitative performance metrics (success rate, rewards, steps)\n",
    "3. Optionally visualize your agent's behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary evaluation tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available scenarios:\n",
      "- simple_static\n",
      "- training_1\n",
      "- training_2\n",
      "- training_3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Add the src directory to the path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import the evaluation tools\n",
    "from src.test_agent_validity import validate_agent, load_agent_class\n",
    "from src.evaluation import evaluate_agent, visualize_trajectory\n",
    "from scenarios import get_scenario, SCENARIOS\n",
    "\n",
    "# List available scenarios\n",
    "print(\"Available scenarios:\")\n",
    "for scenario_name in sorted(SCENARIOS.keys()):\n",
    "    print(f\"- {scenario_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set your evaluation parameters below. You can easily modify these values without changing the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent to evaluate: ../src/agents/agent_trained_example.py\n",
      "Scenario: simple_static\n",
      "Using 5 seeds: [42, 43, 44, 45, 46]\n",
      "Max steps per episode: 200\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Path to your agent implementation (change this to your agent file path)\n",
    "# AGENT_PATH = \"../src/agents/agent_greedy.py\"\n",
    "AGENT_PATH = \"../src/agents/agent_trained_example.py\"\n",
    "\n",
    "# Scenario to evaluate on (choose from the list printed above)\n",
    "SCENARIO_NAME = \"simple_static\"  # Options: simple_static, training_1, training_2, training_3, etc.\n",
    "\n",
    "# Evaluation parameters\n",
    "SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for evaluation\n",
    "MAX_HORIZON = 200            # Maximum steps per episode\n",
    "VERBOSE = True               # Show progress bar\n",
    "RENDER = False               # Enable rendering (slower but necessary for visualization)\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "# Validation and informational prints\n",
    "print(f\"Agent to evaluate: {AGENT_PATH}\")\n",
    "print(f\"Scenario: {SCENARIO_NAME}\")\n",
    "print(f\"Using {len(SEEDS)} seeds: {SEEDS}\")\n",
    "print(f\"Max steps per episode: {MAX_HORIZON}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Validate Agent\n",
    "\n",
    "First, let's load and validate your agent implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully loaded agent: QLearningTrainedAgent\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_agent(agent_path):\n",
    "    \"\"\"Load and validate an agent from a file path.\"\"\"\n",
    "    try:\n",
    "        # Validate the agent first\n",
    "        validation_results = validate_agent(agent_path)\n",
    "        \n",
    "        if not validation_results['valid']:\n",
    "            print(\"❌ Agent validation failed:\")\n",
    "            for error in validation_results['errors']:\n",
    "                print(f\"  - {error}\")\n",
    "            return None\n",
    "        \n",
    "        # If valid, return the agent class\n",
    "        return validation_results['agent_class']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading agent: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load and validate the agent specified in AGENT_PATH\n",
    "AgentClass = load_and_validate_agent(AGENT_PATH)\n",
    "\n",
    "if AgentClass:\n",
    "    print(f\"✅ Successfully loaded agent: {AgentClass.__name__}\")\n",
    "    # Create an instance of your agent\n",
    "    agent = AgentClass()\n",
    "else:\n",
    "    print(\"⚠️ Please fix your agent implementation before evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Specified Scenario\n",
    "\n",
    "Let's evaluate your agent on the scenario you selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating agent on scenario: simple_static\n",
      "Using 5 seeds with max horizon of 200 steps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4607b1372304073a72e818be21b0429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating seeds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Success Rate: 60.00%\n",
      "Mean Reward: 11.46 ± 9.58\n",
      "Mean Steps: 180.0 ± 19.6\n",
      "\n",
      "Individual Episode Results:\n",
      "  Seed 42: Reward=100.0, Steps=151, Success=✓\n",
      "  Seed 43: Reward=100.0, Steps=185, Success=✓\n",
      "  Seed 44: Reward=0.0, Steps=200, Success=✗\n",
      "  Seed 45: Reward=0.0, Steps=200, Success=✗\n",
      "  Seed 46: Reward=100.0, Steps=164, Success=✓\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def print_evaluation_results(results):\n",
    "    \"\"\"Print evaluation results in a readable format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Success Rate: {results['success_rate']:.2%}\")\n",
    "    print(f\"Mean Reward: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}\")\n",
    "    print(f\"Mean Steps: {results['mean_steps']:.1f} ± {results['std_steps']:.1f}\")\n",
    "    \n",
    "    if 'individual_results' in results:\n",
    "        print(\"\\nIndividual Episode Results:\")\n",
    "        for i, episode in enumerate(results['individual_results']):\n",
    "            print(f\"  Seed {episode['seed']}: \" + \n",
    "                  f\"Reward={episode['reward']:.1f}, \" +\n",
    "                  f\"Steps={episode['steps']}, \" +\n",
    "                  f\"Success={'✓' if episode['success'] else '✗'}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "\n",
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Get the selected scenario\n",
    "    scenario = get_scenario(SCENARIO_NAME)\n",
    "    \n",
    "    print(f\"Evaluating agent on scenario: {SCENARIO_NAME}\")\n",
    "    print(f\"Using {len(SEEDS)} seeds with max horizon of {MAX_HORIZON} steps\")\n",
    "    \n",
    "    # Run the evaluation\n",
    "    results = evaluate_agent(\n",
    "        agent=agent,\n",
    "        scenario=scenario,\n",
    "        seeds=SEEDS,\n",
    "        max_horizon=MAX_HORIZON,\n",
    "        verbose=VERBOSE,\n",
    "        render=RENDER,\n",
    "        full_trajectory=True  # Need full trajectory for later visualization\n",
    "    )\n",
    "    \n",
    "    # Display the results\n",
    "    print_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on All Training Scenarios\n",
    "\n",
    "To get a comprehensive evaluation, you can test your agent on all training scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating agent on 3 training scenarios...\n",
      "\n",
      "Scenario: training_1\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 3.21\n",
      "  Mean Steps: 196.6\n",
      "\n",
      "Scenario: training_2\n",
      "  Success Rate: 20.00%\n",
      "  Mean Reward: 3.93\n",
      "  Mean Steps: 192.6\n",
      "\n",
      "Scenario: training_3\n",
      "  Success Rate: 0.00%\n",
      "  Mean Reward: 0.00\n",
      "  Mean Steps: 200.0\n",
      "\n",
      "==================================================\n",
      "OVERALL SUCCESS RATE: 13.33%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Choose which training scenarios to evaluate on\n",
    "TRAINING_SCENARIOS = [\"training_1\", \"training_2\", \"training_3\"]\n",
    "\n",
    "# Evaluation parameters for all scenarios\n",
    "ALL_SEEDS = [42, 43, 44, 45, 46]  # Seeds to use for all evaluations\n",
    "ALL_MAX_HORIZON = 200             # Maximum steps per episode\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "# Only run if the agent was successfully loaded\n",
    "if 'agent' in locals():\n",
    "    # Store results for each scenario\n",
    "    all_results = {}\n",
    "    \n",
    "    print(f\"Evaluating agent on {len(TRAINING_SCENARIOS)} training scenarios...\")\n",
    "    \n",
    "    # Evaluate on each scenario\n",
    "    for scenario_name in TRAINING_SCENARIOS:\n",
    "        print(f\"\\nScenario: {scenario_name}\")\n",
    "        \n",
    "        # Get the scenario\n",
    "        scenario = get_scenario(scenario_name)\n",
    "        \n",
    "        # Run the evaluation\n",
    "        results = evaluate_agent(\n",
    "            agent=agent,\n",
    "            scenario=scenario,\n",
    "            seeds=ALL_SEEDS,\n",
    "            max_horizon=ALL_MAX_HORIZON,\n",
    "            verbose=False,  # Less verbose for multiple evaluations\n",
    "            render=False,\n",
    "            full_trajectory=False\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        all_results[scenario_name] = results\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"  Success Rate: {results['success_rate']:.2%}\")\n",
    "        print(f\"  Mean Reward: {results['mean_reward']:.2f}\")\n",
    "        print(f\"  Mean Steps: {results['mean_steps']:.1f}\")\n",
    "    \n",
    "    # Print overall performance\n",
    "    total_success = sum(r['success_rate'] for r in all_results.values()) / len(all_results)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"OVERALL SUCCESS RATE: {total_success:.2%}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Results Across Scenarios\n",
    "\n",
    "The table below summarizes your agent's performance across all the training scenarios. \n",
    "This gives you a comprehensive view of how well your agent generalizes to different wind patterns and conditions.\n",
    "\n",
    "A strong agent should:\n",
    "1. Maintain a high success rate across all scenarios\n",
    "2. Achieve good rewards efficiently (high reward values)\n",
    "3. Complete episodes in fewer steps (better efficiency)\n",
    "\n",
    "Compare your agent's performance across scenarios to identify potential weaknesses that you might address in future improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Results Across All Scenarios:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Mean Reward</th>\n",
       "      <th>Success Rate</th>\n",
       "      <th>Mean Steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAINING_1</td>\n",
       "      <td>3.21 ± 6.42</td>\n",
       "      <td>20.00%</td>\n",
       "      <td>196.6 ± 6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAINING_2</td>\n",
       "      <td>3.93 ± 7.85</td>\n",
       "      <td>20.00%</td>\n",
       "      <td>192.6 ± 14.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAINING_3</td>\n",
       "      <td>0.00 ± 0.00</td>\n",
       "      <td>0.00%</td>\n",
       "      <td>200.0 ± 0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scenario  Mean Reward Success Rate    Mean Steps\n",
       "0  TRAINING_1  3.21 ± 6.42       20.00%   196.6 ± 6.8\n",
       "1  TRAINING_2  3.93 ± 7.85       20.00%  192.6 ± 14.8\n",
       "2  TRAINING_3  0.00 ± 0.00        0.00%   200.0 ± 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Across Training Scenarios:\n",
      "  Success Rate: 13.33%\n",
      "  Mean Reward: 2.38\n",
      "  Mean Steps: 196.4\n",
      "\n",
      "Note: Your final evaluation will include hidden test scenarios.\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "### SUMMARY TABLE FOR ALL SCENARIOS #########\n",
    "#############################################\n",
    "\n",
    "# Only run if the agent was successfully loaded and evaluated on multiple scenarios\n",
    "if 'agent' in locals() and 'all_results' in locals():\n",
    "    # Create summary table with pandas\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Prepare data for summary table\n",
    "    summary_data = []\n",
    "    for scenario_name, results in all_results.items():\n",
    "        summary_data.append({\n",
    "            'Scenario': scenario_name.upper(),\n",
    "            'Mean Reward': f\"{results['mean_reward']:.2f} ± {results['std_reward']:.2f}\",\n",
    "            'Success Rate': f\"{results['success_rate']:.2%}\",\n",
    "            'Mean Steps': f\"{results['mean_steps']:.1f} ± {results['std_steps']:.1f}\"\n",
    "        })\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display summary table\n",
    "    from IPython.display import display\n",
    "    print(\"\\nSummary of Results Across All Scenarios:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Calculate average across scenarios\n",
    "    avg_success_rate = np.mean([results['success_rate'] for results in all_results.values()])\n",
    "    avg_reward = np.mean([results['mean_reward'] for results in all_results.values()])\n",
    "    avg_steps = np.mean([results['mean_steps'] for results in all_results.values()])\n",
    "    \n",
    "    print(f\"\\nAverage Across Training Scenarios:\")\n",
    "    print(f\"  Success Rate: {avg_success_rate:.2%}\")\n",
    "    print(f\"  Mean Reward: {avg_reward:.2f}\")\n",
    "    print(f\"  Mean Steps: {avg_steps:.1f}\")\n",
    "    print(\"\\nNote: Your final evaluation will include hidden test scenarios.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Agent Behavior (Optional)\n",
    "\n",
    "If you want to see how your agent behaves in a specific scenario, you can visualize its trajectory.\n",
    "First, enable rendering by setting `VISUALIZE = True` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing agent behavior on scenario: training_1\n",
      "Using seed: 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3fc5b329b342539860295291f3d933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Step:', max=199), Output()), _dom_classes=('widget-inter…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############################################\n",
    "### MODIFY THESE PARAMETERS AS NEEDED ######\n",
    "#############################################\n",
    "\n",
    "# Set to True to enable visualization\n",
    "VISUALIZE = True\n",
    "\n",
    "# Visualization parameters\n",
    "VIZ_SCENARIO_NAME = \"training_1\"  # Choose which scenario to visualize\n",
    "VIZ_SEED = 42                    # Choose a single seed for visualization\n",
    "\n",
    "#############################################\n",
    "### DO NOT MODIFY BELOW THIS LINE ##########\n",
    "#############################################\n",
    "\n",
    "# Only run if visualization is enabled and agent is loaded\n",
    "if VISUALIZE and 'agent' in locals():\n",
    "    # Get the scenario with visualization parameters\n",
    "    viz_scenario = get_scenario(VIZ_SCENARIO_NAME)\n",
    "    viz_scenario.update({\n",
    "        'env_params': {\n",
    "            'wind_grid_density': 25,\n",
    "            'wind_arrow_scale': 80,\n",
    "            'render_mode': \"rgb_array\"\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    print(f\"Visualizing agent behavior on scenario: {VIZ_SCENARIO_NAME}\")\n",
    "    print(f\"Using seed: {VIZ_SEED}\")\n",
    "    \n",
    "    # Run the evaluation with visualization enabled\n",
    "    viz_results = evaluate_agent(\n",
    "        agent=agent,\n",
    "        scenario=viz_scenario,\n",
    "        seeds=VIZ_SEED,\n",
    "        max_horizon=MAX_HORIZON,\n",
    "        verbose=False,\n",
    "        render=True,\n",
    "        full_trajectory=True  # Enable full trajectory for visualization\n",
    "    )\n",
    "    \n",
    "    # Visualize the trajectory with a slider\n",
    "    visualize_trajectory(viz_results, None, with_slider=True)\n",
    "else:\n",
    "    if 'agent' in locals():\n",
    "        print(\"Visualization is disabled. Set VISUALIZE = True to see agent behavior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Command-Line Evaluation\n",
    "\n",
    "For quick evaluation of your agent on specific scenarios, you can use the command-line interface:\n",
    "\n",
    "```bash\n",
    "cd Sailing_project_v1/src\n",
    "python3 evaluate_submission.py agents/agent_greedy.py --scenario training_1 --seeds {1..100}\n",
    "```\n",
    "\n",
    "### Command Options\n",
    "\n",
    "- `agents/agent_greedy.py`: Path to your agent implementation file\n",
    "- `--scenario NAME`: Specific scenario to evaluate on (e.g., `training_1`, `training_2`, `training_3`)\n",
    "- `--seeds {1..100}`: Evaluate on seeds 1 through 100 (bash expansion syntax)\n",
    "- `--max_horizon N`: Maximum steps per episode (default: 200)\n",
    "- `--output FILE`: Save results to a JSON file (e.g., `--output results.json`)\n",
    "- `--verbose`: Show detailed evaluation results (default: simplified output)\n",
    "\n",
    "### Sample Output (Simplified)\n",
    "\n",
    "```\n",
    "Validating agent: agents/your_agent.py\n",
    "✅ Successfully loaded agent: YourAgent\n",
    "Evaluating on 1 scenarios with 100 seeds\n",
    "Max horizon: 200 steps\n",
    "SCENARIO | SUCCESS RATE | MEAN REWARD | MEAN STEPS\n",
    "training_1 | Success: 98.00% | Reward: 61.43 ± 3.85 | Steps: 49.2 ± 6.2\n",
    "======================================================================\n",
    "OVERALL | 98.00% ± 0.00% | 61.43 ± 3.85 | 49.2 ± 6.2\n",
    "======================================================================\n",
    "```\n",
    "\n",
    "This simplified output shows the essential metrics:\n",
    "- **Success Rate**: Percentage of successful episodes\n",
    "- **Mean Reward**: Average discounted reward (higher is better)\n",
    "- **Mean Steps**: Average number of steps to reach the goal\n",
    "- **Standard Deviations**: Variability in performance across different seeds\n",
    "\n",
    "For more detailed output, add the `--verbose` flag to see seed-by-seed results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a standardized way to evaluate agents for the Sailing Challenge. You've now:\n",
    "\n",
    "1. Validated your agent's implementation to ensure it meets the interface requirements\n",
    "2. Evaluated your agent on one or more scenarios to measure its performance\n",
    "3. Viewed a summary of your agent's results across multiple scenarios\n",
    "4. Optionally visualized your agent's behavior in a specific scenario\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Fine-tune your agent**: Use the performance metrics to identify areas for improvement\n",
    "- **Test across all scenarios**: Ensure your agent can handle different wind patterns\n",
    "- **Optimize for efficiency**: Aim to reach the goal in fewer steps\n",
    "- **Consider advanced strategies**: Experiment with algorithms that better account for wind physics\n",
    "\n",
    "Remember that your final agent will be evaluated on both the training scenarios and hidden test scenarios, so your agent should be robust and adaptable.\n",
    "\n",
    "Good luck with your agent submission!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
